{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the HuggingFace repository and local path\n",
    "ct_filename = \"ct.nii.gz\"\n",
    "pancreas_segmentations_filename = \"segmentations/pancreas.nii.gz\"\n",
    "\n",
    "data_folder = \"./data\"\n",
    "encoded_images_dir = os.path.join(data_folder, \"encoded_imgs\")\n",
    "\n",
    "os.path.join(encoded_images_dir, 'enc_healthy_cubes.pkl')\n",
    "import pickle\n",
    "\n",
    "with open(os.path.join(encoded_images_dir, 'enc_healthy_cubes.pkl'), 'rb') as f:\n",
    "    enc_healthy_cubes = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(encoded_images_dir, 'enc_pancreatic_tumor_cubes.pkl'), 'rb') as f:\n",
    "    enc_pancreatic_tumor_cubes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc_healthy_cubes.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfortunately some of the CTs have less than 10 samples bcs. they ended up being out of range\n",
    "\n",
    "- [ ] TODO Reduce to min. no. samples of all CTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def process_and_stack_tensors(enc_pancreatic_tumor_cubes):\n",
    "    \"\"\"\n",
    "    Process the tensors in the dictionary:\n",
    "    1. Stack the tensors, ignoring empty lists.\n",
    "    2. Take tensors with at least 5 entries along the first dimension.\n",
    "    3. Threshold them to 5 entries, dropping those with fewer than 5 entries.\n",
    "    4. Stack all the processed tensors along a new axis.\n",
    "\n",
    "    Parameters:\n",
    "    enc_pancreatic_tumor_cubes (dict): The input dictionary with lists of tensors.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A tensor with all the processed tensors stacked along a new axis.\n",
    "    \"\"\"\n",
    "    # Stack the tensors, ignoring empty lists\n",
    "    stacked_inner_lists = [torch.stack(inner_list).squeeze(1) for inner_list in enc_pancreatic_tumor_cubes.values() if inner_list]\n",
    "\n",
    "    # Filter and reduce the tensors to have exactly 5 entries along the first dimension\n",
    "    filtered_and_reduced_tensors = [tensor[:5] for tensor in stacked_inner_lists if tensor.shape[0] >= 5]\n",
    "\n",
    "    # Stack all the processed tensors along a new axis\n",
    "    if filtered_and_reduced_tensors:\n",
    "        final_tensor = torch.stack(filtered_and_reduced_tensors)\n",
    "    else:\n",
    "        final_tensor = torch.tensor([])  # Return an empty tensor if no tensors meet the criteria\n",
    "\n",
    "    return final_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 5, 8, 24, 24, 24])\n",
      "torch.Size([42, 5, 8, 24, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "enc_pancreatic_tensors = process_and_stack_tensors(enc_pancreatic_tumor_cubes)\n",
    "enc_healthy_tensors = process_and_stack_tensors(enc_healthy_cubes)\n",
    "\n",
    "print(enc_pancreatic_tensors.shape)\n",
    "print(enc_healthy_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55, 5, 8, 24, 24, 24])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tensor = torch.cat([enc_healthy_tensors, enc_pancreatic_tensors], dim=0)\n",
    "final_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "healthy_labels = torch.zeros(enc_healthy_tensors.shape[0])\n",
    "pancreatic_labels = torch.ones(enc_pancreatic_tensors.shape[0])\n",
    "\n",
    "final_labels = torch.cat([healthy_labels, pancreatic_labels], dim=0)\n",
    "final_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = final_tensor\n",
    "labels = final_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply scaling and Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_and_scale_features(features):\n",
    "    \"\"\"\n",
    "    Flatten and standardize the features.\n",
    "\n",
    "    Parameters:\n",
    "        features (numpy.ndarray): Original features of shape (n_samples, ...).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Flattened and standardized features of shape (n_samples, n_features_flat).\n",
    "    \"\"\"\n",
    "    n_samples = features.shape[0]\n",
    "    flattened_features = features.reshape(n_samples, -1)\n",
    "    scaler = StandardScaler()\n",
    "    flattened_features_std = scaler.fit_transform(flattened_features)\n",
    "    return flattened_features_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'features' is your data of shape (n_samples, 8, 24, 24, 24)\n",
    "# and 'labels' contains the labels (0 for healthy, 1 for unhealthy)\n",
    "# Load or generate your 'features' and 'labels' here\n",
    "\n",
    "# Flatten and scale features\n",
    "flattened_features_std = flatten_and_scale_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(features, n_components):\n",
    "    \"\"\"\n",
    "    Apply PCA to reduce dimensionality.\n",
    "\n",
    "    Parameters:\n",
    "        features (numpy.ndarray): Input features.\n",
    "        n_components (int): Number of components to retain.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: PCA-transformed features.\n",
    "        float: Total explained variance ratio.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    features_pca = pca.fit_transform(features)\n",
    "    explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "    return features_pca, explained_variance\n",
    "\n",
    "def apply_tsne(features, n_components, perplexity=30):\n",
    "    \"\"\"\n",
    "    Apply t-SNE to reduce dimensionality.\n",
    "\n",
    "    Parameters:\n",
    "        features (numpy.ndarray): Input features.\n",
    "        n_components (int): Target number of dimensions.\n",
    "        perplexity (float): Perplexity parameter for t-SNE.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: t-SNE-transformed features.\n",
    "    \"\"\"\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, random_state=42)\n",
    "    features_tsne = tsne.fit_transform(features)\n",
    "    return features_tsne\n",
    "\n",
    "def apply_umap(features, n_components, n_neighbors=15, min_dist=0.1):\n",
    "    \"\"\"\n",
    "    Apply UMAP to reduce dimensionality.\n",
    "\n",
    "    Parameters:\n",
    "        features (numpy.ndarray): Input features.\n",
    "        n_components (int): Target number of dimensions.\n",
    "        n_neighbors (int): Number of neighbors for UMAP.\n",
    "        min_dist (float): Minimum distance parameter for UMAP.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: UMAP-transformed features.\n",
    "    \"\"\"\n",
    "    umap_reducer = umap.UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        random_state=42\n",
    "    )\n",
    "    features_umap = umap_reducer.fit_transform(features)\n",
    "    return features_umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_evaluate(features, method, n_clusters, **kwargs):\n",
    "    \"\"\"\n",
    "    Perform clustering and evaluate performance.\n",
    "\n",
    "    Parameters:\n",
    "        features (numpy.ndarray): Input features for clustering.\n",
    "        method (str): Clustering method ('kmeans', 'agglomerative', 'gmm').\n",
    "        n_clusters (int): Number of clusters.\n",
    "        **kwargs: Additional keyword arguments for clustering algorithms.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    if method == 'kmeans':\n",
    "        clustering = KMeans(n_clusters=n_clusters, random_state=42, **kwargs)\n",
    "        clusters = clustering.fit_predict(features)\n",
    "    elif method == 'agglomerative':\n",
    "        clustering = AgglomerativeClustering(n_clusters=n_clusters, **kwargs)\n",
    "        clusters = clustering.fit_predict(features)\n",
    "    elif method == 'gmm':\n",
    "        clustering = GaussianMixture(n_components=n_clusters, random_state=42, **kwargs)\n",
    "        clusters = clustering.fit_predict(features)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported clustering method: {method}\")\n",
    "\n",
    "    # Evaluate clustering performance\n",
    "    if len(set(clusters)) > 1 and len(set(clusters)) < len(features):\n",
    "        silhouette_avg = silhouette_score(features, clusters)\n",
    "        calinski_harabasz = calinski_harabasz_score(features, clusters)\n",
    "        davies_bouldin = davies_bouldin_score(features, clusters)\n",
    "    else:\n",
    "        silhouette_avg = calinski_harabasz = davies_bouldin = np.nan\n",
    "\n",
    "    evaluation = {\n",
    "        'Silhouette Score': silhouette_avg,\n",
    "        'Calinski-Harabasz Score': calinski_harabasz,\n",
    "        'Davies-Bouldin Score': davies_bouldin\n",
    "    }\n",
    "    return evaluation\n",
    "\n",
    "\n",
    "def evaluate_clustering(features, labels):\n",
    "    \"\"\"\n",
    "    Evaluate clustering performance across different dimensionality reduction and clustering methods.\n",
    "\n",
    "    Parameters:\n",
    "        features (numpy.ndarray): Original high-dimensional features.\n",
    "        labels (numpy.ndarray): True labels (if available, not used in clustering).\n",
    "\n",
    "    Returns:\n",
    "        dict: Nested dictionaries containing DataFrames of evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Define parameter ranges\n",
    "    n_components_list = list(range(2, 56, 5))  # From 2 to 55 in steps of 5\n",
    "    n_clusters_list = list(range(2, 8))        # From 2 to 7 in steps of 1\n",
    "\n",
    "    dim_reduction_methods = ['PCA', 't-SNE', 'UMAP']\n",
    "    clustering_methods = ['kmeans', 'agglomerative', 'gmm']\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    results = {}\n",
    "\n",
    "    for dim_name in dim_reduction_methods:\n",
    "        print(f\"\\nDimensionality Reduction Method: {dim_name}\")\n",
    "\n",
    "        # Initialize a nested dictionary to store results\n",
    "        results[dim_name] = {}\n",
    "\n",
    "        for n_components in n_components_list:\n",
    "            if dim_name == 't-SNE' and n_components > 3:\n",
    "                continue\n",
    "\n",
    "            print(f\"  n_components = {n_components}\")\n",
    "\n",
    "            # Apply dimensionality reduction\n",
    "            if dim_name == 'PCA':\n",
    "                reduced_features, explained_variance = apply_pca(features, n_components)\n",
    "                print(f\"    Explained Variance Ratio: {explained_variance:.4f}\")\n",
    "            elif dim_name == 't-SNE':\n",
    "                # Adjust perplexity if necessary based on n_components and sample size\n",
    "                reduced_features = apply_tsne(features, n_components=n_components)\n",
    "            elif dim_name == 'UMAP':\n",
    "                reduced_features = apply_umap(features, n_components=n_components)\n",
    "\n",
    "            # Initialize DataFrame for this n_components\n",
    "            key = f\"n_components_{n_components}\"\n",
    "            results[dim_name][key] = pd.DataFrame(\n",
    "                index=n_clusters_list,\n",
    "                columns=clustering_methods\n",
    "            )\n",
    "\n",
    "            for n_clusters in n_clusters_list:\n",
    "                for method in clustering_methods:\n",
    "                    # Perform clustering and evaluation\n",
    "                    evaluation = cluster_and_evaluate(\n",
    "                        reduced_features,\n",
    "                        method=method,\n",
    "                        n_clusters=n_clusters\n",
    "                    )\n",
    "                    # Store Silhouette Score in the DataFrame\n",
    "                    results[dim_name][key].loc[n_clusters, method] = evaluation['Silhouette Score']\n",
    "\n",
    "        print(\"  Evaluation complete.\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation ðŸ§ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensionality Reduction Method: PCA\n",
      "  n_components = 2\n",
      "    Explained Variance Ratio: 0.0840\n",
      "  n_components = 7\n",
      "    Explained Variance Ratio: 0.2058\n",
      "  n_components = 12\n",
      "    Explained Variance Ratio: 0.3135\n",
      "  n_components = 17\n",
      "    Explained Variance Ratio: 0.4137\n",
      "  n_components = 22\n",
      "    Explained Variance Ratio: 0.5102\n",
      "  n_components = 27\n",
      "    Explained Variance Ratio: 0.6004\n",
      "  n_components = 32\n",
      "    Explained Variance Ratio: 0.6866\n",
      "  n_components = 37\n",
      "    Explained Variance Ratio: 0.7697\n",
      "  n_components = 42\n",
      "    Explained Variance Ratio: 0.8466\n",
      "  n_components = 47\n",
      "    Explained Variance Ratio: 0.9179\n",
      "  n_components = 52\n",
      "    Explained Variance Ratio: 0.9793\n",
      "  Evaluation complete.\n",
      "\n",
      "Dimensionality Reduction Method: t-SNE\n",
      "  n_components = 2\n",
      "  Evaluation complete.\n",
      "\n",
      "Dimensionality Reduction Method: UMAP\n",
      "  n_components = 2\n",
      "  n_components = 7\n",
      "  n_components = 12\n",
      "  n_components = 17\n",
      "  n_components = 22\n",
      "  n_components = 27\n",
      "  n_components = 32\n",
      "  n_components = 37\n",
      "  n_components = 42\n",
      "  n_components = 47\n",
      "  n_components = 52\n",
      "  Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation\n",
    "results = evaluate_clustering(flattened_features_std, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for PCA:\n",
      "\n",
      "n_components = 2\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.415781      0.403015  0.412469\n",
      "3  0.471768      0.435367  0.471768\n",
      "4   0.42468      0.325491  0.207349\n",
      "5  0.372024      0.324841  0.187038\n",
      "6  0.369312      0.364407  0.176826\n",
      "7  0.378918      0.365611  0.268334\n",
      "\n",
      "\n",
      "n_components = 7\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.197427      0.178406  0.247093\n",
      "3  0.196978      0.186267  0.243406\n",
      "4  0.207885      0.210924  0.215972\n",
      "5  0.235804       0.23825  0.091066\n",
      "6  0.222924      0.260684  0.091944\n",
      "7  0.245807      0.264427  0.100665\n",
      "\n",
      "\n",
      "n_components = 12\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.128265      0.125084   0.18264\n",
      "3   0.13986      0.129365 -0.040489\n",
      "4  0.152924      0.130982 -0.005983\n",
      "5  0.138891      0.133962  0.000402\n",
      "6  0.143016      0.156038  0.006684\n",
      "7  0.147451      0.154797  0.009358\n",
      "\n",
      "\n",
      "n_components = 17\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.101461      0.135077  0.112626\n",
      "3    0.1046      0.135731  0.115402\n",
      "4  0.136785      0.136128  0.083596\n",
      "5  0.088546      0.116673  0.088546\n",
      "6   0.10559      0.122613  0.090307\n",
      "7  0.098124       0.10166  0.098124\n",
      "\n",
      "\n",
      "n_components = 22\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.086011       0.09898  0.081577\n",
      "3  0.080407      0.101154  0.080407\n",
      "4  0.086329        0.0992  0.086329\n",
      "5  0.117829      0.098654  0.060899\n",
      "6  0.065317      0.078688  0.065317\n",
      "7   0.13112      0.076356   0.06182\n",
      "\n",
      "\n",
      "n_components = 27\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.088016      0.098628  0.088016\n",
      "3  0.068502      0.100197  0.089277\n",
      "4   0.09013       0.09084   0.09013\n",
      "5  0.041764      0.093564  0.041764\n",
      "6  0.045498      0.094987  0.045498\n",
      "7  0.019935         0.098  0.019935\n",
      "\n",
      "\n",
      "n_components = 32\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.017781      0.082575  0.065162\n",
      "3  0.042034      0.083465  0.068359\n",
      "4  0.024402      0.082544  0.024402\n",
      "5  0.073782      0.082989  0.023745\n",
      "6   0.16545      0.084116  0.027008\n",
      "7  0.066387      0.036812 -0.003287\n",
      "\n",
      "\n",
      "n_components = 37\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.028258      0.065857  0.076995\n",
      "3  0.025288      0.066929  0.073227\n",
      "4  0.044697      0.066321  0.075719\n",
      "5  0.065465      0.045232  0.072934\n",
      "6  0.052688      0.033225  0.065929\n",
      "7  0.067985      0.035525  0.067985\n",
      "\n",
      "\n",
      "n_components = 42\n",
      "     kmeans agglomerative       gmm\n",
      "2 -0.009368       0.04191  0.042857\n",
      "3  0.018068      0.043242   0.04573\n",
      "4  0.048179       0.03259  0.048179\n",
      "5  0.101684      0.034521  0.050634\n",
      "6  0.029569      0.018238  0.029569\n",
      "7  0.031816      0.020319  0.031816\n",
      "\n",
      "\n",
      "n_components = 47\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.002438      0.041594  0.084654\n",
      "3  0.077819      0.042476  0.086079\n",
      "4  0.032704      0.037453  0.084813\n",
      "5  0.033795      0.038698  0.077905\n",
      "6  0.009575      0.037193  0.057304\n",
      "7  0.025998      0.027801  0.043343\n",
      "\n",
      "\n",
      "n_components = 52\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.009644      0.029569  0.058029\n",
      "3 -0.002366      0.030108  0.059611\n",
      "4  0.028814      0.025133  0.060782\n",
      "5  0.057053      0.026162  0.057053\n",
      "6  0.038161      0.024796  0.049372\n",
      "7  0.037982      0.017562  0.037982\n",
      "\n",
      "\n",
      "\n",
      "Results for t-SNE:\n",
      "\n",
      "n_components = 2\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.333609      0.286493  0.353892\n",
      "3  0.353235       0.29542  0.321182\n",
      "4  0.350649       0.28754  0.324073\n",
      "5  0.340967      0.303123  0.321626\n",
      "6  0.331515      0.315315  0.023125\n",
      "7   0.35002      0.329798  0.208338\n",
      "\n",
      "\n",
      "\n",
      "Results for UMAP:\n",
      "\n",
      "n_components = 2\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.392187      0.355898  0.392187\n",
      "3  0.393041      0.377982  0.393041\n",
      "4  0.342489      0.312147  0.342858\n",
      "5  0.314689      0.275356  0.329916\n",
      "6  0.314829       0.25788  0.262747\n",
      "7  0.303352      0.265273  0.144586\n",
      "\n",
      "\n",
      "n_components = 7\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.270217      0.219544  0.268809\n",
      "3  0.223316      0.219939   0.21913\n",
      "4  0.227542      0.207699  0.214941\n",
      "5   0.22297      0.211697  0.178856\n",
      "6   0.21317      0.198436  0.195344\n",
      "7  0.203148      0.201555  0.203148\n",
      "\n",
      "\n",
      "n_components = 12\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.253236       0.23366  0.235342\n",
      "3  0.250102      0.242059  0.185435\n",
      "4  0.226184      0.220598  0.170787\n",
      "5  0.183472       0.21496  0.181165\n",
      "6  0.200196      0.183797  0.141862\n",
      "7  0.167293       0.18599   0.10873\n",
      "\n",
      "\n",
      "n_components = 17\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.243623      0.204299  0.242057\n",
      "3  0.196844      0.214757  0.193481\n",
      "4  0.188305      0.186456  0.188305\n",
      "5  0.184691      0.177786  0.157667\n",
      "6  0.163817      0.151155   0.12018\n",
      "7  0.156329      0.160968    0.1284\n",
      "\n",
      "\n",
      "n_components = 22\n",
      "     kmeans agglomerative       gmm\n",
      "2   0.24149       0.21014   0.24149\n",
      "3  0.237351      0.227375  0.197826\n",
      "4  0.206264      0.202077   0.19474\n",
      "5  0.170715       0.18525  0.186558\n",
      "6  0.204878       0.13368  0.147859\n",
      "7  0.168821      0.153692   0.16476\n",
      "\n",
      "\n",
      "n_components = 27\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.250534      0.216181  0.252021\n",
      "3   0.21125      0.233423  0.203768\n",
      "4  0.221084      0.205574  0.189924\n",
      "5  0.206518      0.206342  0.165824\n",
      "6  0.200921      0.200573   0.14531\n",
      "7   0.16075      0.158131  0.136073\n",
      "\n",
      "\n",
      "n_components = 32\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.252662      0.236272  0.248029\n",
      "3  0.243313      0.226582   0.20317\n",
      "4  0.229702      0.209934  0.171533\n",
      "5  0.167721      0.198228  0.162981\n",
      "6  0.187478      0.148821  0.155753\n",
      "7  0.185655      0.157428  0.169818\n",
      "\n",
      "\n",
      "n_components = 37\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.266444      0.240097  0.265702\n",
      "3  0.254769      0.244584  0.192099\n",
      "4  0.221092      0.206189  0.207686\n",
      "5  0.196398      0.163757   0.17583\n",
      "6  0.168288      0.168183  0.163623\n",
      "7  0.165844      0.179113  0.161069\n",
      "\n",
      "\n",
      "n_components = 42\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.252201      0.241236  0.247343\n",
      "3  0.250581      0.231664  0.195181\n",
      "4  0.219725      0.198211  0.209637\n",
      "5  0.200433      0.185144  0.191632\n",
      "6  0.168306      0.189071  0.148645\n",
      "7  0.155853      0.178322  0.155853\n",
      "\n",
      "\n",
      "n_components = 47\n",
      "     kmeans agglomerative       gmm\n",
      "2  0.251986      0.222621   0.24858\n",
      "3  0.246647      0.242266  0.190145\n",
      "4  0.214047      0.201409  0.160783\n",
      "5  0.207761       0.21892  0.170639\n",
      "6  0.183652      0.192649  0.158247\n",
      "7  0.178018       0.20782  0.157862\n",
      "\n",
      "\n",
      "n_components = 52\n",
      "     kmeans agglomerative       gmm\n",
      "2   0.25163       0.23362   0.25163\n",
      "3   0.24208      0.236792  0.202722\n",
      "4  0.223867      0.210106  0.218086\n",
      "5  0.201307      0.199843   0.18392\n",
      "6  0.178506      0.206642  0.178506\n",
      "7  0.172476      0.188018  0.156653\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the results for each dimensionality reduction method and n_components\n",
    "for dim_name, components_dict in results.items():\n",
    "    print(f\"\\nResults for {dim_name}:\\n\")\n",
    "    for n_components_key, df in components_dict.items():\n",
    "        print(f\"n_components = {n_components_key.split('_')[-1]}\")\n",
    "        print(df)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results ðŸ“Š\n",
    "\n",
    "PCA Optimal Number of Clusters:\n",
    "\tâ€¢\tFor n_components = 2, n_clusters = 3 yields the highest Silhouette Score (0.4718) with K-Means, suggesting that three clusters may better represent the underlying data structure in this reduced space.\n",
    "\n",
    "\n",
    "t-SNE Optimal Number of Clusters:\n",
    "\n",
    "â€¢\tK-Means with n_clusters = 3 gives a Silhouette Score of 0.3532, indicating that three clusters might be a good choice in t-SNE reduced space.\n",
    "\n",
    "UMAP:\n",
    "Optimal Number of Clusters:\n",
    "\tâ€¢\tWith n_components = 2, K-Means with n_clusters = 3 gives a Silhouette Score of 0.3930."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
